{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'><tr>\n",
    "    <td style='background-color:red; text-align:center; color: white;'><!--Foundation<!--hr size='5' style='border-color:red; background-color:red;'--></td>\n",
    "    <td style='background-color:yellow; text-align:center;'><!--Level 1<!--hr size='5' style='border-color:yellow; background-color:yellow;'--></td>\n",
    "    <td style='background-color:orange; text-align:center;'><!--Level 2<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:green; text-align:center; color: white;'><!--Level 3<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:blue; text-align:center; color: white;'><!--Level 4<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:purple; text-align:center; color: white;'><!--Level 5<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:brown; text-align:center; color: white;'><!--Level 6<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:black; text-align:center; color: white;'><!--Level 7<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style='border-left:10px solid orange;'><tr>\n",
    "    <td style='padding-left:20px;'>\n",
    "        <h2><i>Swansea University Medical School</i><br/><b>MSc Health Data Science</b></h2>\n",
    "        <h3>PMIM-102 Introduction to Scientific Computing in Healthcare</h3>\n",
    "        <h1><b>Introduction to Programming in R</b></h1>\n",
    "        <h2><b>2. Programming with Statistics</b></h2>\n",
    "        <h2><i>Part 3: Hypothesis testing.</i></h2>\n",
    "        <h3><i>September 2020</i></h3>\n",
    "    </td>\n",
    "    <td><img height='300' width='500' src='images/cover.jpg'/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Aim__: Explore facilities in R for doing statistical analyses\n",
    "\n",
    "The aim of this session is to build upon the R basics of the previous session to explore the tools available in R for statistical analysis.\n",
    "\n",
    "### __A map of where we're going__\n",
    "\n",
    "1. <b>Probability and probability distributions</b> and plotting in R - uniform, binomial, poisson. The Central Limit Theorem.\n",
    "\n",
    "1. <b>Descriptive statistics</b> - the normal distribution, expected value, independence, mean, variance, summary.\n",
    "\n",
    "1. <div style=\"background-color:yellow;\"><b>Hypothesis Testing</b> - confidence interval, standard error, p-value, degrees of freedom, (non) parametric.</div>\n",
    "\n",
    "1. <b>Single Sample Analysis</b> - what is the mean value, is it what was expected, what is its uncertainty?\n",
    "\n",
    "1. <b>Comparing Two (or More) Samples</b> - comparing means, variances, frequencies and distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Statistical Inference 2: Hypothesis Testing__ (Significance Tests)\n",
    "\n",
    "As well as testing the precision of an estimate, we can test the strength of the evidence that the data provides about some proposition of interest (Bland).\n",
    "\n",
    "A _good_ hypothesis is one which can be refuted, for example, the hypothesis that there are _no_ zombies in the local area can be refuted by finding a zombie whereas the hypothesis that there are vampires abroad at night cannot be refuted as you cannot be sure you have looked everywhere.\n",
    "\n",
    "### The Null and Alternative Hypotheses\n",
    "\n",
    "There are two hypotheses involved in a significance test. The first, the __null hypothesis__ (sometimes written, h0) is that there is no difference or no effect in the treatments. The __alternative hypothesis__ (sometimes written hA) is that there is a difference in the treatments (in one direction or the other).\n",
    "\n",
    "We examine the data we have and determine what the probability is that we could get data as extreme as this if the null hypothesis was in fact true. If the probability is high, then the data are consistent with the null hypothesis and there is no evidence that there is any difference. On the other hand, if the probability is low, the data is not consistent with the null hypothesis and we  can say that there is some evidence that the alternative hypothesis is true.\n",
    "\n",
    "### Test Statistics\n",
    "\n",
    "A test statistic is a measurement we make on the sample. The test statistic is then compared to a known distribution which it would follow if the null hypothesis were true. We then calculate the probability that a value of the test statistic might occur which is more extreme than the observed value if the null hypothesis were true. From this we can conclude whether the data are consistent or inconsistent with the null hypothesis.\n",
    "\n",
    "If the data are not consistent with the null hypothesis, the difference is __statistically significant__.\n",
    "\n",
    "### P-value\n",
    "\n",
    "The _p_-value is the probability that value of the test statistic could have occurred if the null hypothesis were true. It is __not__ the probability that the null hypothesis is true etc. The p-value is usually given as a number, e.g. p=0.00345. We often look for significance as having a _p_-value less than, for example, 0.05 (i.e. a 95% chance that the data could not have occurrred if the null hypothesis were true) but sometimes you will see p<0.01 or lower (note that the probability is sometimes referred to as the significance level - but the significance level is considered high if the _p_-value is low).\n",
    "\n",
    "It is also very important not to think that p<0.05 is a magic number and that p<0.07 should be ignored. Such results should still be considered when interpreting your statistics as they may be indicative of interacting effects or incomplete elimination of confounders.\n",
    "\n",
    "### Errors\n",
    "\n",
    "There are two kinds of errors:\n",
    "* type I or α error where we decide to reject a true null hypothesis (a false positive); and\n",
    "* type II or β error where we decide to not to reject a false null hypothesis (a false negative).\n",
    "\n",
    "The more stringent we are in our demand of the significance level, the larger the difference we need between data that supports and rejects the null hypothesis, the more likely we are to miss real differences. So reducing the risk of a Type I error, increases the risk of a Type II error.\n",
    "\n",
    "### One/Two sided significance\n",
    "\n",
    "A one sided-test (or tailed) is one in which we are only interested if there is a difference in one direction, e.g. that there is an improvement in the health of patients, not if there is an improvement _or_ worsening in their condition. In the one-sided case, for p<0.05, we only need to reach a value of the statistic above which 5% of the distribution lies. In the two sided case, we need to go further to find the value above which 2.5% lies as well as the 2.5% which would have a more extreme result in the worsening direction.\n",
    "\n",
    "### Significant, real and important\n",
    "\n",
    "Even if we generate a significant test result, the effect may not be important if, for example, the effect is not very large - i.e. we have shown that there probably is an effect, but not that it will actually make a difference to anyone. But also, we may not have significant data but the effect is still really there; we may not have had the right sample or a big enough sample to see a significant difference.\n",
    "\n",
    "For this reason it is not considered acceptable to merely report the p-value as proof of an effect. It is generally preferred that some form of effect size is reported, for example, a confidence interval.\n",
    "\n",
    "### Multiple Tests - Warning!\n",
    "\n",
    "[XKCD 882](https://xkcd.com/882/)\n",
    "\n",
    "If we test a null hypothesis which is, in fact, true, we have a 95% chance of coming to the 'not significant' conclusion. If we perform two tests, the probability of getting 'not significant' twice is 0.95 x 0.95 = 0.90. So, if we do 20 tests, there is a ${0.95}^{20}$ chance of getting a 'not significant' conclusion (0.36) i.e. a 0.64 chance of getting an erroneous 'significant' conclusion at least once. The expected number of spurious results is 20 x 0.05 = 1.\n",
    "\n",
    "The _Bonferroni_ method of analysing multiple tests allows for this and requires that, for _k_ tests with a significance level of $\\alpha$ we need to look for significance in _any_ of the tests at the significance level of $\\alpha/k$. For example, for 5 tests at the 0.05 significance level, we consider significance to have been reached if any one test has p<0.01. Note this means that the overall test still has a significance level of 0.05 not 0.01.\n",
    "\n",
    "### Degrees of Freedom\n",
    "\n",
    "This is the sample size minus the number of parameters estimated from the sample. So, if you have a sample of 10 values and calulate the mean we have 9 degrees of freedom if we use the difference between the values and the mean.\n",
    "\n",
    "### Power of a Test\n",
    "\n",
    "The probability that a particular test will predict a significant difference at a given significance level is the __power__ of the test. This will depend on the true difference between the populations, the sample size and the significance level.\n",
    "Generally accepted significance levels are $\\alpha$ = 0.05 (the probability of a false positive) and $\\beta$ = 0.2 (the probability of a false negative). The power of a test is defined as:\n",
    "\n",
    "\\begin{equation*} power = 1 -\\beta \\end{equation*}\n",
    "\n",
    "This is used to calculate the sample sizes necessary to detect a specified difference when the error variance is known. For a Normal distribution see Bland. If we look ahead to the comparison of two sample means: this is governed by Student's _t_ distribution using the test statistic: difference / standard-error-of-the-difference gives a sample size of:\n",
    "\n",
    "\\begin{equation*} n = \\frac {2s^2t^2}{d^2} \\end{equation*}\n",
    "\n",
    "where s is the standard deviation, t the value of the test statistic (summed for both errors) and d the required difference you need to distinguish (Crawley, 2014).\n",
    "\n",
    "We can get R to perform this calculation with `power.t.test()`, where `delta` is the difference we wish to detect, `sd` is the standard deviation and `power` is the required power (with the default `sig.level=0.95`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#power.t.test(delta=2, sd=2.8, power=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power calculations are also available for anova (`power.anova.test`) and proportion tests (`power.prop.test`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric and Non-parametric\n",
    "\n",
    "Tests that rely on the assumption that the data have come from a Normal distribution are `Parametric Tests`. Those that do not need such a restriction are `Non-Parametric`. The advantage of parametric tests is that they are more powerful and so require smaller sample sizes to produce accurate results. However, the Normality restriction can often not be met or is unknown and so we are forced to use the less powerful non-parametric tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Parametric Tests: Probability Distributions Used in Statistical Tests_\n",
    "\n",
    "There are 3 distributions commonly encountered in statistical testing:\n",
    "\n",
    "1. Student's _t_-distribution\n",
    "1. _F_-distribution\n",
    "1. _$\\chi^2$_ distribution\n",
    "\n",
    "The _t_-test and _F_-test are used for continuous data to test means of small samples and variances and the _$\\chi^2$_-test is used to test independence in contingency tables and distribution _goodness of fit_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Non-parametric Statistical Tests Using Rank Order Instead of Distributions_\n",
    "\n",
    "Statistical tests which do not rely on a probability distribution are described as non-parametric. They are more widely appplicable but less powerful than the parametric tests. They are based on the concept of Rank Order.\n",
    "\n",
    "1. Sign test\n",
    "1. Wilcoxon Test\n",
    "1. Mann Whitney U test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Tests\n",
    "\n",
    "Often we analyse two independent samples, for example, we may examine whether consumption of energy drinks improves performance at zombie shoot-em-up games by randomly selecting two groups of people and giving one group the energy drink and the other a placebo. This test results in unpaired scores. If, on the other hand, we used the same sample and tested them twice, once with energy drinks and once with the placebo, the scores would not be independent and would be paired improving the power of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;\"><tr><td width=\"100\" height=\"20\" style=\"background-color:greenyellow\"></td><td width=\"100\" height=\"20\" style=\"background-color:hotpink\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __What Tests and When to Do Them__\n",
    "\n",
    "The three most common problems in statistical inference are (Bland):\n",
    "1. Comparison of one group under different conditions (Section 4)\n",
    "1. Comparison of two independent groups (Section 5)\n",
    "1. Investigation of the relationship of two variables measured on the same sample of subjects --- see the modelling section later.\n",
    "\n",
    "Bland provides a useful table suggesting which test you should use according to the number of samples and the type of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for Analysis of One or a Paired Sample\n",
    "\n",
    "| Type of Data     | Size of Sample            | Method                          |\n",
    "|------------------|---------------------------|---------------------------------|\n",
    "| Interval         | Large (>100)              | Normal distribution             |\n",
    "|                  | Small (<100), non-normal  | Wilcoxon matched-pairs test     |\n",
    "|                  | Small (<100), normal data | Student's _t_-test              |\n",
    "| Ordinal          | Any                       | Sign test                       |\n",
    "| Nominal, ordered | Any                       | Sign test                       |\n",
    "| Nominal          | Any                       | Stuart test *                   |\n",
    "| Dichotomous      | Any                       | McNemar's test                  |\n",
    "|\\* Unusual and difficult to do.|\n",
    "\n",
    "### Methods for Comparing Two Samples\n",
    "\n",
    "| Type of Data     | Size of Sample            | Method                          |\n",
    "|------------------|---------------------------|---------------------------------|\n",
    "| Interval         | Large (>50 each)          | Normal distribution for means   |\n",
    "|                  | Small (<50), non-normal   | Mann-Whitney U-test             |\n",
    "|                  | Small (<50), normal data &| Two sample _t_-test             |\n",
    "|                  |              uniform var  |                                 |\n",
    "| Ordinal          | Any                       | Mann-Whitney U-test             |\n",
    "| Nominal, ordered | Large (>30)               | Chi-squared for trend           |\n",
    "| Nominal, unordered | Large, most expected n>5 | Chi-squared                    |\n",
    "|                  | Small, more than 20% with | Reduce categories               |\n",
    "|                  | expected n<5              |                                 |\n",
    "| Dichotomous      | Large, all expected n>5   | Comparison of two proportions   |\n",
    "|                  |                           | Chi-squared test                |\n",
    "|                  |                           | Odds ratio                      |\n",
    "|                  | Small, at least one expected n<5 | Chi-squared test with Yates' correction |\n",
    "|                  |                                  | Fisher's exact test      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Size Considerations\n",
    "\n",
    "#### _Non-normal Distributions_\n",
    "\n",
    "Data are likely to deviate either in skewness or in grouping. Grouping occurs when we round data resulting in a stepped distribution rather than a continuous distribution. This doesn't tend to affect the application of t (or even Normal) statistics very much.\n",
    "\n",
    "Skewness is a more serious problem. It is less so in paired data as the subtraction has a normalising effect and similarly is worse for the single-sample test than the two-sampled test (especially if the two samples are of similar size). The single sample test may fail (insufficient power) to detect differences or may have a wide confidence interval.\n",
    "\n",
    "The other weakness occurs with differences in variance between samples, but this is also associated with skewness, so fixing that usually fixes problems with variance too.\n",
    "\n",
    "If normalising does not work, use non-parametric methods.\n",
    "\n",
    "#### _When is a sample a large sample?_\n",
    "\n",
    "Since the value of the t-statistic for 30 samples (30 df) at 95% is 2.04, it is close enough that we can use the Normal values instead for samples of Normal data with uniform variance. If small (or if you are not sure, assume small), attempt transformations to create a Normal distribution (in the paired case, transform before subtraction).\n",
    "\n",
    "The more non-Normal your data, the larger the sample will need to be before the parametric statistics will approximate to the data. Bland suggests that the level at which you can assume Normal approximations are close enough is approximately 100, but is also a matter of experience and judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;\"><tr><td width=\"100\" height=\"20\" style=\"background-color:greenyellow\"></td><td width=\"100\" height=\"20\" style=\"background-color:hotpink\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'><tr>\n",
    "    <td style='background-color:red; text-align:center; color: white;'><!--Foundation<!--hr size='5' style='border-color:red; background-color:red;'--></td>\n",
    "    <td style='background-color:yellow; text-align:center;'><!--Level 1<!--hr size='5' style='border-color:yellow; background-color:yellow;'--></td>\n",
    "    <td style='background-color:orange; text-align:center;'><!--Level 2<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:green; text-align:center; color: white;'><!--Level 3<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:blue; text-align:center; color: white;'><!--Level 4<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:purple; text-align:center; color: white;'><!--Level 5<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:brown; text-align:center; color: white;'><!--Level 6<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:black; text-align:center; color: white;'><!--Level 7<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Bibliography__\n",
    "\n",
    "Martin Bland, An Introduction to Medical Statistics, OUP.\n",
    "\n",
    "Michael Crawley, Statistics: An Introduction Using R, Wiley.\n",
    "\n",
    "Paul Teetor, R Cookbook, O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
