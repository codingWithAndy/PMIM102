{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'><tr>\n",
    "    <td style='background-color:red; text-align:center; color: white;'><!--Foundation<!--hr size='5' style='border-color:red; background-color:red;'--></td>\n",
    "    <td style='background-color:yellow; text-align:center;'><!--Level 1<!--hr size='5' style='border-color:yellow; background-color:yellow;'--></td>\n",
    "    <td style='background-color:orange; text-align:center;'><!--Level 2<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:green; text-align:center; color: white;'><!--Level 3<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:blue; text-align:center; color: white;'><!--Level 4<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:purple; text-align:center; color: white;'><!--Level 5<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:brown; text-align:center; color: white;'><!--Level 6<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:black; text-align:center; color: white;'><!--Level 7<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style='border-left:10px solid orange;'><tr>\n",
    "    <td style='padding-left:20px;'>\n",
    "        <h2><i>Swansea University Medical School</i><br/><b>MSc Health Data Science</b></h2>\n",
    "        <h3>PMIM-102 Introduction to Scientific Computing in Healthcare</h3>\n",
    "        <h1><b>Introduction to Programming in R</b></h1>\n",
    "        <h2><b>2. Programming with Statistics</b></h2>\n",
    "        <h2><i>Part 5: Comparing two samples.</i></h2>\n",
    "        <h3><i>September 2020</i></h3>\n",
    "    </td>\n",
    "    <td><img height='300' width='500' src='images/cover.jpg'/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Aim__: Explore facilities in R for doing statistical analyses\n",
    "\n",
    "The aim of this session is to build upon the R basics of the previous session to explore the tools available in R for statistical analysis.\n",
    "\n",
    "### __A map of where we're going__\n",
    "\n",
    "1. <b>Probability and probability distributions</b> and plotting in R - uniform, binomial, poisson. The Central Limit Theorem.\n",
    "\n",
    "1. <b>Descriptive statistics</b> - the normal distribution, expected value, independence, mean, variance, summary.\n",
    "\n",
    "1. <b>Hypothesis Testing</b> - confidence interval, standard error, p-value, degrees of freedom, (non) parametric..\n",
    "\n",
    "1. <b>Single Sample Analysis</b> - what is the mean value, is it what was expected, what is its uncertainty?\n",
    "\n",
    "1. <div style=\"background-color:yellow;\"><b>Comparing Two (or More) Samples</b> - comparing means, variances, frequencies and distributions.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Comparing Two Samples__\n",
    "\n",
    "* Compare Two Variances\n",
    "* Compare two sample means with normal errors (t.test)\n",
    "* Compare two sample means with non-normal errors (wilcox.test)\n",
    "* Compare two proportions (prop.test, binom.test)\n",
    "* Testing for independence in a contingency table (chisq.test)\n",
    "* Testing small samples for correlation (fisher.test)\n",
    "* Correlating two variables (cor.test)\n",
    "* Pairwise Comparison of Means for Several Samples\n",
    "* Do Two Samples Come From the Same Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compare Two Variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often want to compare the variances of two samples before we use the _t_-test since it is only applicable if the variances are similar.\n",
    "\n",
    "To compare the variance of two samples assuming that they are Normally distributed, we use the fact that the ratios of two independent estimates of the same variance follows an F distribution. This is a little involved as the F-distribution can't be integrated and tables require two degrees of freedom (one for each sample). However, the test can be simply performed in R with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var.test(x, y, alternative = \"two.sided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a small _p_-value, the probability of getting this extreme a result with the null hypothesis (the variances are the same) is small, so we reject it and deduce that the two variances are unlikely to be equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Background: The _F_-Distribution__\n",
    "\n",
    "*Background - you can skip this section.*\n",
    "\n",
    "The F-distribution is the distribution followed by the ratio of variances and can be used to test variances of Normally distributed, similarly spread samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fisher - Snedecor distribution with _m_ and _n_ degrees of freedom is the distribution of:\n",
    "\n",
    "\\begin{equation*} \\frac { \\frac { \\chi_m^2 }{m} }{ \\frac { \\chi_n^2 }{n} }  \\end{equation*}\n",
    "\n",
    "Again, remembering this is not necessary, just that there are two $\\chi^2$ distributions and two degrees of freedom involved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exercise__: Explore the _F_-distribution functions in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "options(repr.plot.width = 6, repr.plot.height = 6)\n",
    "#colours <- c(\"red\", \"green\", \"orange\", \"purple\", \"blue\", \"black\", \"grey\", \"pink\")\n",
    "#x <- seq(1, 30, 1)\n",
    "#y <- seq(1, 30, 5)\n",
    "#plot(x, qf(0.975, df1=x, df2=1), type='l', col='red')\n",
    "#lines(x, qf(0.975, df1=x, df2=2), type='l', col='blue')\n",
    "#plot(x, qf(0.975, df1=1, df2=x), type='l', col='red')\n",
    "#lines(x, qf(0.975, df1=5, df2=x), type='l', col='blue')\n",
    "\n",
    "#plot(c(0, 4), c(0, 2), type='n')\n",
    "#x <- seq(0, 4, 0.05)\n",
    "#for (d1 in y){\n",
    "#   for (d2 in y){\n",
    "#       lines(x, df(x, df1=d1, df2=d2), type='l', col='red')\n",
    "#   }\n",
    "#}\n",
    "#lines(x, dnorm(x, mean=1.0, sd=0.2))\n",
    "\n",
    "#sequence <- c(1, 2, 5, 10, 15, 20, 30)\n",
    "#plot(c(0, 4), c(0, 1.0), type='n')\n",
    "#x <- seq(0, 4, 0.1)\n",
    "#for (d1 in sequence){\n",
    "#   for (d2 in sequence){\n",
    "#       lines(x, pf(x, df1=sequence[d1], df2=sequence[d2]), type='l', col=colours[d1])\n",
    "#   }\n",
    "#}\n",
    "#legend(\"bottomright\", legend=sequence, fill=colours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compare Two Sample Means (Parametric - t-Test)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse several means, use __ANOVA__ (see later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both samples need to be large enough, say >20 samples, __or__ Normally distributed (or at least with a bell-shaped distribution \\[Teetor, p212\\]). The test also depends on whether the samples are _paired_ or independent.\n",
    "\n",
    "The test statistic is the number of standard errors by which the two sample means are separated:\n",
    "\n",
    "\\begin{equation*} t = \\frac {difference \\; between \\; the \\; two \\; means}{standard \\; error \\; of \\; the \\; difference} = \\frac {\\bar{y_A} - \\bar{y_B}}{SE_{diff}} \\end{equation*}\n",
    "\n",
    "where the standard error of the difference between the two sample means is:\n",
    "\n",
    "\\begin{equation*} SE_{diff} = \\sqrt { \\frac {{s_A}^2}{n_A} + \\frac {{s_B}^2}{n_B} } \\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x <- rnorm(100)\n",
    "#y <- rnorm(100) + 0.4\n",
    "#head(x)\n",
    "#head(y)\n",
    "#t.test(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _With Paired Data_\n",
    "\n",
    "The variance of a difference is given by:\n",
    "\n",
    "\\begin{equation*} (y_A - \\mu_A)^2 + (y_B - \\mu_B)^2 - 2(y_A - \\mu_A)(y_B- \\mu_B) \\end{equation*}\n",
    "\n",
    "i.e. the variance is reduced by any positive covariance between variables A and B. If we pair readings this may well be due to their being correlated (taken from the same person or in the same location) and this will reduce the variance and improve the power of the test statistic.\n",
    "\n",
    "You could also do a single-sample _t_-test on the difference between the pairs of readings and get a similar result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.test(x, y, paired=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compare Two Sample Means (Non-Parametric - Wilcoxon Rank Sum Test)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the relative location of two samples of similar but unknown (or non-Normal) distribution, you can use the Wilcoxon-Mann-Whitney test (use `paired=TRUE` for paired observations). The null hypothesis is that the two samples are located at the same place. If the _p_-value is <0.05 we can conclude that they are not.\n",
    "\n",
    "The test works by placing the labelled samples into a single array. The samples are then sorted and given a rank. Ties are given the average rank. The ranks are then summed for each of the labels and the rank-sum of the smaller compared to the Wilcoxon rank sum tables. We reject the null hypothesis is the rank sum is smaller than the value in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x <- rbinom(100, size=10, prob=0.1) * 50\n",
    "#y <- rnorm(100) * 50 + 10\n",
    "#wilcox.test(x, y, paired=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compare Two (or more) Proportions_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to compare two proportions to determine if they are significantly different, you can use the _n_-sample test for equality of proportions. You need to provide a pair with the success counts and a pair with the total sample counts. A _p_-value < 0.05 indicates that the two proportions are likely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prop.test(c(1,10), c(7,160))\n",
    "#prop.test(c(1,10,8,88), c(7,160,88,99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Independence in Contingency Tables_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have two categorical variables (nominal, not-ordered), represented by factors, you can test them for independence with the _$\\chi^2$_-test. You first create a contingency table and then run a summary on it.\n",
    "The null hypothesis is that there is no association between the variables (i.e. that they are independent). A small _p_-value, therefore, would suggest that the variables are probably not independent and that there is a connection between them.\n",
    "\n",
    "This is a large-sample test, the smaller the expected values become the less reliable the test. Conventionally, it is considered OK if 80% of the expected frequencies exceed 5 and all exceed 1. But this may be conservative. If these criterion are not met, you can combine and/or delete rows and columns until they are met or you are reduced to a 2 x 2 table.\n",
    "\n",
    "If you still do not meet the criterion, you can apply a continuity correction or use Fisher's Exact Test (see Non-parametric tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df <- data.frame(crew_member=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20),\n",
    "#                shirt=c('red', 'yellow', 'blue', 'red', 'yellow', 'blue', 'red', 'red',\n",
    "#                        'blue', 'red', 'blue', 'yellow', 'blue', 'red', 'yellow', 'red',\n",
    "#                        'yellow', 'blue', 'yellow', 'blue'),\n",
    "#                surname=c(FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE,\n",
    "#                          FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE))\n",
    "#tb <-table(df$shirt, df$surname)\n",
    "#tb\n",
    "#summary(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This (Pearson's $\\chi^2$) works by calculating, for all the entries in the table, the sum of the squares of the difference between the observed and expected values divided by the expected values:\n",
    "\n",
    "\\begin{equation*} \\sum { \\frac { (O - E)^2 }{ E }}  \\end{equation*}\n",
    "\n",
    "If this calculated value is greater than the value of the _$\\chi^2$_ statistic at the critical value (e.g. 95%) we reject the null hypothesis (that the two variables are independent). If the observed value is greater than the expected value then the two variables are _positively_ associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Challenge__: Calculate the Pearson's _$\\chi^2$_ for the red-shirt table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Background: The _$\\chi^2$_-Distribution__\n",
    "\n",
    "*Again, this is background and, although interesting, can be skip or lightly skimmed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exercise__: Explore the _$\\chi^2$_-distribution functions in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "#colours <- c(\"red\", \"green\", \"orange\", \"purple\", \"blue\", \"black\", \"grey\", \"pink\")\n",
    "#x <- seq(1, 10, 1)\n",
    "#plot(x, qchisq(0.975, df=x), type='l', col='red')\n",
    "\n",
    "#x <- seq(0, 10, 1)\n",
    "#plot(dchisq(x, df=1), type='l', main=\"Chi-squared Distribution\")\n",
    "\n",
    "#x <- seq(0, 10, 0.1)\n",
    "#plot(c(0, 10), c(0, 0.5), type='n')\n",
    "##plot(x, dchisq(x, df=1), type='l', col=colours[d])\n",
    "#sequence <- c(1, 2, 5, 10, 15, 20, 30) \n",
    "#for (d in 1:length(sequence)){\n",
    "#   lines(x, dchisq(x, df=sequence[d]), type='l', col=colours[d])\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Small Samples Correlation (Fisher's Exact Test)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a contingency table in which one or more of the expected frequencies is less than 5, use Fisher's Exact Test. This works by calculating the probability of all the possible combinations and summing those which are the outcomes of interest and seeing if the probability of the event is greater than the required p-value. If it is we cannot conclude that the null hypothesis is not valid.\n",
    "\n",
    "As well as the table data, you could provide the fisher.test() function with two vectors containing the original data and it will work out the table itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the redshorts table form above.\n",
    "#fisher.test(tb)\n",
    "## For a basic table.\n",
    "#tb <- as.matrix(c(6, 4, 2, 8))\n",
    "#dim(tb) <- c(2, 2)\n",
    "#fisher.test(tb)\n",
    "## From the data vectors.\n",
    "#fisher.test(df$shirt, df$surname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Correlation Between Two Variables_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between two variables provides us with an indication of how closely one variable varies with another (NB correlation does not imply causation).\n",
    "\n",
    "The correlation coefficient is defined as the ratio of the covariance (the expectation of the vector product of the variables) with the geometric mean of the individual variances:\n",
    "\n",
    "\\begin{equation*} r = \\frac {cov(x, y)}{\\sqrt {s_x^2 s_y^2}} \\end{equation*}\n",
    "\n",
    "The expectation of the vector product isn't as bad as it sounds, it is:\n",
    "\n",
    "\\begin{equation*} cov(x, y) = E(xy) - E(x)E(y) \\end{equation*}\n",
    "\n",
    "You can determine the correlation between two variable with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x <- runif(100)\n",
    "#y <- x + runif(100) * 4\n",
    "#cor(x, y)\n",
    "#plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the calculation manually(ish) (since var(x) is the variance of x and var(x, y) is the covariance of x and y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var(x, y) / sqrt(var(x) * var(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the variables are Normally distributed, you can use the Pearson method to calculate the _p_-value and confidence interval of the correlation. If they are not Normally distributed, use the non-parametric Spearman method or Kendall's Tau. A _p_-value < 0.05 indicates that the correlation is significant. If the confidence interval contains zero, it is entirely possible that the correlation value could be zero i.e. that the variables are not, in fact, correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cor.test(x, y)\n",
    "#cor.test(x, y, method='spearman')\n",
    "#cor.test(x, y, method='kendall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exercise__: Try a few experiments with the values for x and y - different distributions, different sample sizes, different constants and see how the correlation coefficient varies and the p-value that is reported. You could also try a few exact and noisy relationships such as:\n",
    "* $x^2$\n",
    "* $x^2$ + rnorm(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Pairwise Comparison of Means for Several Samples_\n",
    "\n",
    "If you have a number of samples and you wish to compare the mean of every sample against every other sample. This is not, in fact, as straightforward as simply going through the process of comparing the sample means which turns out to be overly optimistic. The _p_-values need to be adjusted. This is not trivial, so consult the help pages for the way this adjustment is made and refer to a good textbook.\n",
    "\n",
    "You need to combine the data into a data frame with data vector and a factor to define the groupings: use the `stack()` function.\n",
    "\n",
    "Large _p_-values and we can't reject the null hypothesis that the means are similar. Small _p_-values and we can assume that the two sample means are significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1 <- rnorm(100)+.1\n",
    "#v2 <- rnorm(100)+.2\n",
    "#v3 <- rnorm(100)\n",
    "#samples <- stack(list(v1=v1, v2=v2, v3=v3))\n",
    "#pairwise.t.test(samples$values, samples$ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Do Two Samples Come From the Same Distribution_\n",
    "\n",
    "You can check that a sample matches another or matches a reference sample you create with one of:\n",
    "* $\\chi^2$ test\n",
    "* Kolmogorov-Smirnov test\n",
    "* Cramér–von Mises criterion\n",
    "\n",
    "Use the non-parametric Kolmogorov-Smirnov test to test whether two samples are likely to have come from the same distribution. A _p_-value of < 0.05 suggests that the two samples were not drawn from the same distribution. Greater than 0.05 provides no evidence. The KS test checks the location, dispersion and shape of the populations based on the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chi-squared test.\n",
    "#x <- rnorm(10000)\n",
    "#y <- rnorm(10000)\n",
    "#chisq.test(x, y, rescale.p=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x <- rnorm(100)\n",
    "#y <- rt(100, df=100)\n",
    "#mean(x)\n",
    "#mean(y)\n",
    "#ks.test(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;\"><tr><td width=\"100\" height=\"20\" style=\"background-color:greenyellow\"></td><td width=\"100\" height=\"20\" style=\"background-color:hotpink\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __ANOVA / ANCOVA__\n",
    "\n",
    "### ANOVA\n",
    "\n",
    "Anova is the technique to use to examine the effects of explanatory variables which are all categorical (factors). We will be analysing the variance of the response variable according to the groupings by the values of the explanatory variables.\n",
    "\n",
    "ANOVA measures the variance and deduces the relative locations of the means which might sound like magic, but is quite straightforward. First calculate the variance for all the data about the entire sample mean. Then calculate the variances for each value of the categorical variable about it's own sample mean. If the category variances are the same as the entire sample variance, the category and whole sample have similar means. If the variance when measured about the category mean is smaller when compared to the whole-sample variance this indicates that the means for each category must be separated.\n",
    "\n",
    "In the event that you want to do a one-way ANOVA non-parametrically, look at the Kruskal-Wallis test.\n",
    "\n",
    "The following code illustrates the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oneway <- read.csv(\"data/oneway.csv\")\n",
    "# attach(oneway)\n",
    "# names(oneway)\n",
    "# par(mfrow=c(1,2))\n",
    "# options(repr.plot.width = 6, repr.plot.height = 6)\n",
    "# plot(1:20, ozone, ylim=c(0,8), ylab='y', xlab='order', pch=21, bg='red')\n",
    "# abline(h=mean(ozone), col='blue')\n",
    "# for(i in 1:20) lines(c(i, i), c(mean(ozone), ozone[i]), col='green')\n",
    "\n",
    "# plot(1:20, ozone, ylim=c(0,8), ylab='y', xlab='order', pch=21, bg=as.numeric(garden))\n",
    "# abline(h=mean(ozone[garden=='A']), col='blue')\n",
    "# abline(h=mean(ozone[garden=='B']), col='red')\n",
    "# for(i in 1:length(ozone)) {\n",
    "#     if (garden[i]=='A')\n",
    "#         lines(c(i, i), c(mean(ozone[garden=='A']), ozone[i]), col='blue')\n",
    "#     else\n",
    "#         lines(c(i, i), c(mean(ozone[garden=='B']), ozone[i]), col='red')\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par(mfrow=c(2,2))\n",
    "# options(repr.plot.width = 8, repr.plot.height = 6)\n",
    "# summary(aov(ozone~garden))\n",
    "# plot(aov(ozone~garden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANCOVA\n",
    "\n",
    "ANCOVA is a combination of regression and analysis of variance. This is used when the response variable is continuous and the explanatory variables include at least one continuous and one categorical variable. Typically it involves estimating a slope and intercept (the regression) for each level of the categorical variable (the ANOVA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;\"><tr><td width=\"100\" height=\"20\" style=\"background-color:greenyellow\"></td><td width=\"100\" height=\"20\" style=\"background-color:hotpink\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'><tr>\n",
    "    <td style='background-color:red; text-align:center; color: white;'><!--Foundation<!--hr size='5' style='border-color:red; background-color:red;'--></td>\n",
    "    <td style='background-color:yellow; text-align:center;'><!--Level 1<!--hr size='5' style='border-color:yellow; background-color:yellow;'--></td>\n",
    "    <td style='background-color:orange; text-align:center;'><!--Level 2<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:green; text-align:center; color: white;'><!--Level 3<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:blue; text-align:center; color: white;'><!--Level 4<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:purple; text-align:center; color: white;'><!--Level 5<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:brown; text-align:center; color: white;'><!--Level 6<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:black; text-align:center; color: white;'><!--Level 7<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Bibliography__\n",
    "\n",
    "Martin Bland, An Introduction to Medical Statistics, OUP.\n",
    "\n",
    "Michael Crawley, Statistics: An Introduction Using R, Wiley.\n",
    "\n",
    "Paul Teetor, R Cookbook, O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
