{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'><tr>\n",
    "    <td style='background-color:red; text-align:center; color: white;'><!--Foundation<!--hr size='5' style='border-color:red; background-color:red;'--></td>\n",
    "    <td style='background-color:yellow; text-align:center;'><!--Level 1<!--hr size='5' style='border-color:yellow; background-color:yellow;'--></td>\n",
    "    <td style='background-color:orange; text-align:center;'><!--Level 2<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:green; text-align:center; color: white;'><!--Level 3<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:blue; text-align:center; color: white;'><!--Level 4<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:purple; text-align:center; color: white;'><!--Level 5<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:brown; text-align:center; color: white;'><!--Level 6<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:black; text-align:center; color: white;'><!--Level 7<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style='border-left:10px solid orange;'><tr>\n",
    "    <td style='padding-left:20px;'>\n",
    "        <h2><i>Swansea University Medical School</i><br/><b>MSc Health Data Science</b></h2>\n",
    "        <h3>PMIM-102 Introduction to Scientific Computing in Healthcare</h3>\n",
    "        <h1><b>Introduction to Programming in R</b></h1>\n",
    "        <h2><b>2. Programming with Statistics</b></h2>\n",
    "        <h2><i>Part 2: The normal distribution and descriptive statistics.</i></h2>\n",
    "        <h3><i>September 2020</i></h3>\n",
    "    </td>\n",
    "    <td><img height='300' width='500' src='images/cover.jpg'/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Aim__: Explore facilities in R for doing statistical analyses\n",
    "\n",
    "The aim of this session is to build upon the R basics of the previous session to explore the tools available in R for statistical analysis.\n",
    "\n",
    "### __A map of where we're going__\n",
    "\n",
    "1. <b>Probability and probability distributions</b> and plotting in R - uniform, binomial, poisson. The Central Limit Theorem.\n",
    "\n",
    "1. <div style=\"background-color:yellow;\"><b>Descriptive statistics</b> - the normal distribution, expected value, independence, mean, variance, summary.</div>\n",
    "\n",
    "1. <b>Hypothesis Testing</b> - confidence interval, standard error, p-value, degrees of freedom, (non) parametric.\n",
    "\n",
    "1. <b>Single Sample Analysis</b> - what is the mean value, is it what was expected, what is its uncertainty?\n",
    "\n",
    "1. <b>Comparing Two (or More) Samples</b> - comparing means, variances, frequencies and distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __The Normal Distribution__\n",
    "\n",
    "The Normal distribution, also known as the Gaussian distribution is given by the equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\phi(x) = \\frac{1}{\\Bigl(\\sqrt{2 \\pi}\\Bigr)} exp \\Bigl(\\frac{-x^2}{2}\\Bigr)\n",
    "\\end{equation*}\n",
    "\n",
    "Which cannot be integrated so has to be numerically solved or determined from tables.\n",
    "\n",
    "Note that the qnorm and dnorm functions assume a mean of zero and standard deviation of 1.\n",
    "\n",
    "When considering the quantiles, if you are wanting the single tailed (the 5% that exceed all other values i.e. either 5% with the greatest values (or the 5% with the smallest values)) select the 95% quantile, but for the two-tailed (the 5% furthest from the mean in either direction i.e. both the 2.5% with the greatest values and the 2.5% with the smallest values) select the 97.5% quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qnorm(0.95)\n",
    "#qnorm(0.975)\n",
    "#dnorm(1.64)\n",
    "#dnorm(1.96)\n",
    "#pnorm(1.96, lower.tail=TRUE)\n",
    "#pnorm(1.96, lower.tail=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "## The probability density and cumulative density distributions.\n",
    "#x <- seq(-5, 5, 0.1)\n",
    "#y <- dnorm(x)\n",
    "#plot(x, y, main=\"Normal Density Distribution\")\n",
    "#y <- pnorm(x)\n",
    "#plot(x, y, main=\"Normal Cumulative Distribution\")\n",
    "## What a sample looks like.\n",
    "#n <- 10000\n",
    "#sample <- rnorm(n, mean=0, sd=1)\n",
    "#plot(sample)\n",
    "## A histogram of the sample.\n",
    "#x <- seq(-4, 4, 0.1)\n",
    "#hist(sample, breaks=50)\n",
    "## Overlay an appropriately scaled normal distribution.\n",
    "#pd <- (n/5)*dnorm(x, mean(sample), sd(sample))\n",
    "#lines(x, pd, col=\"red\")\n",
    "## Then draw in the upper tail (5%).\n",
    "## Not because this is something you will need to do, but\n",
    "## to demonstrate how simple this kind of thing can be.\n",
    "#xpoly <- x[x>=1.70]\n",
    "#ypoly <- pd[x>=1.70]\n",
    "#xpoly <- c(xpoly, 1.70)\n",
    "#ypoly <- c(ypoly, 0)\n",
    "#polygon(xpoly, ypoly, col=\"yellow\")\n",
    "## Then draw in the upper and lower tails (2.5% each).\n",
    "#xpoly <- x[x>=2.00]\n",
    "#ypoly <- pd[x>=2.00]\n",
    "#xpoly <- c(xpoly, 2.00)\n",
    "#ypoly <- c(ypoly, 0)\n",
    "#polygon(xpoly, ypoly, col=\"green\")\n",
    "#xpoly <- x[x<=-2.00]\n",
    "#ypoly <- pd[x<=-2.00]\n",
    "#xpoly <- c(xpoly, -2.00)\n",
    "#ypoly <- c(ypoly, 0)\n",
    "#polygon(xpoly, ypoly, col=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Random Numbers\n",
    "\n",
    "Each distribution has a random sampling function given by `rname` where name is the name of the distribution (e.g. `rnorm()`, `rpois()` etc.). This actually produces a sequence of numbers which is precisely defined by the algorithm so is really pseudo-random. You can change/define the sequence by setting the random generator seed to a specific value - which means you can create the same random sequence if you wish to. If you want to generate a more random sequence you need to set the seed value more randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Discussion__: How can we generate the initial seed value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,3))\n",
    "#n <- 1000\n",
    "#sample <- rnorm(n, mean=0, sd=1)\n",
    "#plot(sample, main='Normal Plot')\n",
    "#boxplot(sample)\n",
    "#qqnorm(sample)\n",
    "#qqline(sample, lty=2)\n",
    "#sample <- rnorm(n, mean=0, sd=1) * rnorm(n, mean=0, sd=1)\n",
    "#plot(sample, main='Normal x Normal Plot')\n",
    "#boxplot(sample)\n",
    "#qqnorm(sample)\n",
    "#qqline(sample, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Other Distributions in R__\n",
    "\n",
    "We have seen four distributions so far, there are many more, though most are somewhat specialist. Later we will have a quick look at Student's t, the F distribution and the chi-squared distribution as these all feature in the definition of the tests we'll cover.\n",
    "\n",
    "In R, for a distribution called 'name', you can expect the following functionality to work:\n",
    "\n",
    "| Name | Function |\n",
    "|---|---|\n",
    "| dname | The density function |\n",
    "| pname | The cumulative density function |\n",
    "| qname | The quantile function |\n",
    "| rname | Random values from the distribution |\n",
    "\n",
    "The following, common, distributions are available:\n",
    "\n",
    "| Distribution | Name | Parameters |\n",
    "|---|---|---|\n",
    "| _Discrete Distributions_ |\n",
    "| Binomial | binom | n=number of trials, p=probability of success for one trial |\n",
    "| Geometric | geom | p=probability of success for one trial |\n",
    "| Hypergeometric | hyper | m=number of white balls in urn, n=number of bllack balls in urn, k=number of balls drawn |\n",
    "| Negative binomial | nbinom | size=number of successful trials, eithe prob= probability of successful trial or mu=mean |\n",
    "| Poisson | pois | lambda=mean |\n",
    "| _Continuous Distributions_ |\n",
    "| Beta | beta | shape1, shape2 |\n",
    "| Cauchy | cauchy | location, scale |\n",
    "| Chi-squared | chisq | df=degrees of freedom |\n",
    "| Exponential | exp | rate |\n",
    "| F | f | df1, df2 |\n",
    "| Gamma | gamma | rate or scale |\n",
    "| Log-normal | lnorm | meanlog=mean on a logarithmic scale, sdlog=standard deviation on a log scale |\n",
    "| Logistic | logis | location, scale |\n",
    "| Normal | norm | mean, sd |\n",
    "| Student's _t_ | t | df |\n",
    "| Uniform | unif | min=lower limit, max=upper limit |\n",
    "| Weibull | weibull | shape, scale |\n",
    "| Wilcoxon | wilcox | m=number of observations in first sample, n=number of observations in second sample |\n",
    "\n",
    "You can get a lot of help from the help function, e.g. `?norm` and from [CRAN task view for probability distributions](https://cran.r-project.org/web/views/Distributions.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;\"><tr><td width=\"100\" height=\"20\" style=\"background-color:greenyellow\"></td><td width=\"100\" height=\"20\" style=\"background-color:hotpink\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Description of a Variable__\n",
    "\n",
    "### _What is a Sample?_\n",
    "\n",
    "A sample is a set of measurements taken from a selected subgroup of a population. One of the issues we have is whether that sample is likely to be representative of the population as a whole. For example, if we measure the mean of our sample, is it reasonable to deduce that the mean of the population is the same?\n",
    "\n",
    "So we can have a look at what a random sample that does come from a known distribution looks like for a number of different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "## Take a sample of 'n' values from a Normal distribution and display the frequency plot.\n",
    "#n <- 10000\n",
    "#sample <- rnorm(n, mean=1, sd=1)\n",
    "#hist(sample)\n",
    "#sample <- runif(n) # + runif(n) + runif(n) + runif(n)\n",
    "#hist(sample)\n",
    "#sample <- rbinom(n, size=1, prob=0.5) # size = 500\n",
    "#hist(sample)\n",
    "#sample <- rpois(n, lambda=0.5) # + rpois(n, lambda=0.5) + rpois(n, lambda=0.5)\n",
    "#hist(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we take a sample of data from a population, we will calculate the mean for that sample. In this case, we are creating a population with a mean of zero and then taking samples from that. You'll see how the small number of samples in each of the following plots result in less convincing Normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(1,1))\n",
    "#population <- rnorm(1000000, mean=0, sd=1)\n",
    "#hist(population)\n",
    "#text <- paste(round(mean(population), 3), '\\n', sep='')\n",
    "#print(text)\n",
    "#text(0, 0, labels=text, col='red', cex=2)\n",
    "#par(mfrow=c(4, 5))\n",
    "#n <- 100\n",
    "## Let's just look at 20 plots\n",
    "#means <- vector()\n",
    "#count <- 20\n",
    "#for (i in 1:count) {\n",
    "#    # I have set the mean here to be 0.16 as an exploration to find out\n",
    "#    # what value I need to generate 1 in 20 means that are less than\n",
    "#    # zero when we have a sample size of 100.\n",
    "#    sample <- rnorm(n, mean=0.16, sd=1)\n",
    "#    hist(sample)\n",
    "#    text <- paste(round(mean(sample), 3), '\\n', sep='')\n",
    "#    print(text)\n",
    "#    means <- append(means, mean(sample))\n",
    "#    text(0, 0, labels=text, col='red', cex=2)\n",
    "#}\n",
    "#par(mfrow=c(1,1))\n",
    "#hist(means, main=paste('Histogram of', count, 'means', sep=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exercise__: Vary the size of the _count_ and see how large you think you need to make it to approach a Normal distribution.\n",
    "\n",
    "The distribution of the means of all the possible samples is known as the __sampling distribution__. If we repeat the acquisition of sample means with many more we can see where it is heading. You'll find that as you increase the number of samples you take to calculate each mean, the standard deviation of the calculated means (the sampling distribution) reduces according to:\n",
    "\n",
    "\\begin{equation*} \\sigma_{means} = \\sqrt{\\frac {\\sigma^2}{n}} \\end{equation*}\n",
    "\n",
    "where $\\sigma$ is the standard deviation of the population (see below). So, the more samples we have, the more likely we are that our sample mean will be close to the population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n <- 400\n",
    "#pop_mean <- 0.16\n",
    "#pop_sd <- 1\n",
    "#means <- vector()\n",
    "#count <- 20000\n",
    "#for (i in 1:count) {\n",
    "#    sample <- rnorm(n, mean=pop_mean, sd=pop_sd)\n",
    "#    means <- append(means, mean(sample))\n",
    "#}\n",
    "#hist(means, breaks=seq(-3,3,0.05), main=paste('Histogram of', count, 'means', sep=' '),\n",
    "#     sub=paste('SD=', sqrt(pop_sd^2/n), sep=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting Expected Values, Means and Variances\n",
    "Some basic terminology - we have a sample which we can use in calculations; we want to be able to understand from the samples the nature of the population(s):\n",
    "\n",
    "* $\\mu$ (mu) is used to denote the mean of the population\n",
    "* $\\sigma$ (sigma) is used to denote the standard deviation of the population\n",
    "* $\\sigma^2$ is the variance of the population\n",
    "\n",
    "\n",
    "* $\\bar{y}$ is the mean of the sample\n",
    "* $s$ is the standard deviation of the sample\n",
    "* $s^2$ is the variance of the sample\n",
    "\n",
    "We have a number of values in our sample. We can calculate a representative value (the average value in the long run, for example) for the sample as the average or mean:\n",
    "\n",
    "\\begin{equation*} mean = \\bar{y} = \\frac {\\sum y}{n} \\end{equation*}\n",
    "\n",
    "In some cases we might have data that is more appropriate to a geometric mean or harmonic mean (see Crawley for more information).\n",
    "\n",
    "A measure of the variability of those values is one of the most useful measures in statistics. In any sample, we can calculate the mean value of the sample ($\\bar{y}$), but by definition, the variation or the sum of all the differences of all the samples from this value is zero. But only because we are taking account of the sign of the difference. We could use the modulus of the difference:\n",
    "\n",
    "\\begin{equation*} \\sum |y - \\bar{y}| \\end{equation*}\n",
    "\n",
    "But, as it turns out, this makes the maths hard. If, instead, we take the square of the differences from the mean, this will be positive for all samples and will give us a sense of the variability of the sample. This is known as the _sum of squares_.\n",
    "\n",
    "\\begin{equation*} \\sum (y - \\bar{y})^2 \\end{equation*}\n",
    "\n",
    "The problem here is that the sum of squares increases with the number of samples we have when it should converge to a fixed value. We could divide by the sample size, but this leads to a slight bias since we have already used one of the sample values (one degree of freedom) to calculate $\\bar{y}$, and we find that we need to divide by (n - 1) and we call this the variance:\n",
    "\n",
    "\\begin{equation*} variance = s^2 = \\frac {\\sum (y - \\bar{y})^2}{(n-1)} \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Statistical Inference 1: Estimation of a Variable__\n",
    "\n",
    "When we have a sample of data it has parameters such as expected values, means and variance. From this we often wish to deduce the parameters for the population from which the sample is drawn. There are two measures of the precision or range of values this estimate might reasonably relate to: _standard error_ and _confidence interval_. These are often reported by statistical functions.\n",
    "\n",
    "### Standard Error\n",
    "\n",
    "The standard error of an estimate of a population mean is defined as the standard deviation of the sampling distribution of the mean (i.e. the distrbution of the mean values of multiple samples of size, _n_).\n",
    "This is given by:\n",
    "\n",
    "\\begin{equation*} \\sqrt { \\frac {\\sigma^2}{n} } \\approx \\sqrt { \\frac {s^2}{n} } \\end{equation*}\n",
    "\n",
    "Where $\\sigma^2$ is the population variance and $s^2$ is the variance estimate. __It provides an indication of the likelihood that the sample mean is close to the true mean.__ Generally speaking the estimate is likely to be within one standard error of the true mean and unlikely to be more than two standard errors from it (Bland).\n",
    "Use the error to indicate the precision of the measurement of the mean and the standard deviation to indicate the spread of the values (although the two are related).\n",
    "\n",
    "With large samples (n) the sampling distribution of the mean $\\bar {x}$ tends toward the population mean, $\\mu$, and $s^2$ is a good estimate of $\\sigma^2$, but for small sample sizes this is not the case.\n",
    "\n",
    "Although the standard error is a standard deviation (of the sampling distribution), it is not the standard deviation of the sample. In general, if you wish to refer to the precision of your estimate use the standard error, and use the standard deviation to describe the variability of the samples, populations or distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n <- 4\n",
    "#means <- vector()\n",
    "#count <- 20000\n",
    "#for (i in 1:count) {\n",
    "#    sample <- rnorm(n, mean=0.16, sd=1)\n",
    "#    means <- append(means, mean(sample))\n",
    "#}\n",
    "#hist(means, breaks=seq(-3,3,0.05), main=paste('Histogram of', count, 'means', sep=' '),\n",
    "#sub=paste('mean=', mean(sample), ', mean(means)=', mean(means), ', SE=', sqrt(var(sample)/n), sep=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Confidence Interval\n",
    "\n",
    "The confidence interval is the likely range in which the mean would fall if the sampling exercise was repeated. We usually refer to the 95% confidence interval which is the range within which the mean should fall 95% of the time. This is related to the standard error by the _t_-distibution (see section 3). With a large sample, we can assume our means are distributed Normally and that, for 95% of samples, the mean will lie in the range of values between the sample mean +/-1.96 times the standard error. The values of the mean at those points are called the confidence limits. This is often written as, for example, 'the mean episode survival time for a red-shirt is 10 minutes +/- 0.4 (95% CI, n=16)'.\n",
    "\n",
    "If the sample size is large enough, you can calculate the CI from the value of the 97.5th percentile (from the inverse cumulative density function) as:\n",
    "\n",
    "\\begin{equation*} CI = q_{norm(97.5)} \\times \\sqrt{ \\frac{\\sigma^2}{n}}  \\end{equation*}\n",
    "\n",
    "But, with a small sample, we will need to use Student's t-distribution:\n",
    "\n",
    "\\begin{equation*} CI = q_{t(97.5)} \\times \\sqrt{ \\frac{\\sigma^2}{n}}  \\end{equation*}\n",
    "\n",
    "Or, if we want to be lazy (and we usually do), we can use the CI() function in the Rmisc library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qnorm(0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_size <- 32\n",
    "#population_mean <- 0\n",
    "#population_sd <- 1\n",
    "#conf_level <- 0.95\n",
    "#sample <- rnorm(n=sample_size, mean=population_mean, sd=population_sd)\n",
    "#se <- sqrt(sd(sample)^2 / sample_size)\n",
    "## Calculate it from the normal distribution.\n",
    "#q_norm <- qnorm(1 - (1 - conf_level)/2) # Two tailed.\n",
    "#ci <- q_norm * se\n",
    "#cat('(normal) mean =', mean(sample), ', ', conf_level * 100, '% CI =', mean(sample) - ci, 'to',\n",
    "#     mean(sample) + ci, 'n =', sample_size, '\\n', sep=' ')\n",
    "## Calculate it with Student's t distribution.\n",
    "#q_t <- qt(1 - (1 - conf_level)/2, df=sample_size - 1) # Two tailed.\n",
    "#ci <- q_t * se\n",
    "#cat('(t)      mean =', mean(sample), ', ', conf_level * 100, '% CI =', mean(sample) - ci, 'to',\n",
    "#     mean(sample) + ci, 'n =', sample_size, '\\n', sep=' ')\n",
    "## Use the CI function from the Rmisc library.\n",
    "#library(Rmisc)\n",
    "#CI(sample, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_size <- 32\n",
    "#root_n <- sqrt(sample_size)\n",
    "#population_mean <- 0\n",
    "#population_sd <- 1\n",
    "#dsamples <- cbind(i=1, n=rnorm(n=sample_size, mean=population_mean, sd=population_sd))\n",
    "#for (i in 2:20) {\n",
    "#    dsamples <- rbind(dsamples, data.frame(i=i, n=rnorm(n=sample_size, mean=population_mean, sd=population_sd)))\n",
    "#}\n",
    "#head(dsamples)\n",
    "#length(dsamples$i)\n",
    "#summary(dsamples$n)\n",
    "#s <- sd(dsamples$n)\n",
    "## Plot the samples.\n",
    "#boxplot(data=dsamples, n~i, main = \"Comparision of samples\", col='orange', border='darkgreen', notch=FALSE)\n",
    "## Mark the lines which are +/- 1.96 standard errors above and below the population mean.\n",
    "#abline(h=c(0, 1.96*(s/root_n), -1.96*(s/root_n)))\n",
    "#conf.int(dsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center;\"><tr><td width=\"100\" height=\"20\" style=\"background-color:greenyellow\"></td><td width=\"100\" height=\"20\" style=\"background-color:hotpink\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'><tr>\n",
    "    <td style='background-color:red; text-align:center; color: white;'><!--Foundation<!--hr size='5' style='border-color:red; background-color:red;'--></td>\n",
    "    <td style='background-color:yellow; text-align:center;'><!--Level 1<!--hr size='5' style='border-color:yellow; background-color:yellow;'--></td>\n",
    "    <td style='background-color:orange; text-align:center;'><!--Level 2<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:green; text-align:center; color: white;'><!--Level 3<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:blue; text-align:center; color: white;'><!--Level 4<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:purple; text-align:center; color: white;'><!--Level 5<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:brown; text-align:center; color: white;'><!--Level 6<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "    <td style='background-color:black; text-align:center; color: white;'><!--Level 7<!--hr size='5' style='border-color:orange; background-color:orange;'--></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Bibliography__\n",
    "\n",
    "Martin Bland, An Introduction to Medical Statistics, OUP.\n",
    "\n",
    "Michael Crawley, Statistics: An Introduction Using R, Wiley.\n",
    "\n",
    "Michael Crawley, The R Book, Wiley."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
